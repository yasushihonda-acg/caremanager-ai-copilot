# 12_phase4_expert_meeting.md - Phase 4 専門家会議議事録

## 会議概要
*   **日時:** 2025年11月18日
*   **テーマ:** AIによるアセスメント自動化（オートマッピング）とインタビュー支援機能について
*   **参加者:** プロジェクトマネージャー、リードエンジニア、ベテランケアマネジャー（佐藤）、AIアーキテクト

## 1. 提起された要望 (User Voice)
> 「録音した内容で各チェック項目に自動でチェックが入るようにしてほしい。そうすれば、画面を見ずに利用者と向き合う（Eye Contact）ことが可能となる。また、聞き漏らしを防ぐためのアドバイスも欲しい。」

## 2. As-Is / To-Be 分析

### As-Is (現状)
*   **入力スタイル:** ケアマネジャーが利用者の話を聞きながら、手動でタブを切り替え、該当するボタン（例：「自立」「一部介助」）をタップしている。
*   **音声機能:** 録音終了後、AIが「特記事項」用の長い要約テキストを生成するのみ。構造化データ（チェックボックス）には反映されない。
*   **課題:** 入力操作のために視線がタブレットに落ちがちになる。会話に集中できない。

### To-Be (あるべき姿)
*   **入力スタイル:** ケアマネジャーはスマホを机に置き、会話に集中する。
*   **音声機能:** AIが会話内容を解析し、23項目のチェックボックス（構造化データ）を自動的に埋める（Auto-Mapping）。
*   **インタビュー支援:** 会話の途中で「金銭管理について聞いていません」「虐待のリスクチェックが必要です」といったアドバイスをAIが画面に表示する（Co-pilot）。

## 3. 技術的課題と解決策
*   **課題:** 自然会話（「トイレは自分で行けるけど、ズボンの上げ下げがちょっとね…」）を、システムの固定選択肢（「一部介助」）にどうマッピングするか？
*   **解決策:** Gemini APIの **Structured Output (JSON Schema)** を利用する。プロンプト内で「介護認定調査基準」の定義を与え、AIに判断（推論）させる。

## 4. 決定事項
1.  **AI Auto-Assessmentの実装:** 音声データを解析し、`AssessmentData` 型に適合するJSONを返却させる機能を実装する。
2.  **Co-pilotの実装:** 解析結果に基づき、「まだ情報が得られていない重要項目」を特定し、次に行うべき質問をサジェストする。
3.  **UIの改修:** 自動入力された項目がわかるように、UI上でハイライト（「AI入力」バッジ等）を行う。

## 5. 次のステップ
*   `docs/13_phase4_design.md` にて、具体的なデータフローとプロンプト設計を定義する。